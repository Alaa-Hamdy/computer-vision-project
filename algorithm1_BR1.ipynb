{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Computer Vision Project</h1>\n",
    "<h2>Phase 1</h2>\n",
    "<h3>Team 3</h3>\n",
    "<ul><li>Anas Salah</li>\n",
    "<li> Alaa Hamdy </li>\n",
    "<li>Ahmed Amr </li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import libraries and packages used</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import cv2\n",
    "import imutils\n",
    "import json\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import ops\n",
    "from operator import itemgetter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load training images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "true_boxes = []\n",
    "\n",
    "picsFolder_path = \"train/train/\"\n",
    "with open('digitStruct.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# import colored pictures\n",
    "for i in range(len(data)):\n",
    "    image = cv2.imread(picsFolder_path + data[i]['filename'])\n",
    "    images.append(image)\n",
    "    temp=[]\n",
    "    for j in range(len(data[i]['boxes'])):\n",
    "        temp.append(data[i]['boxes'][j]['label'])\n",
    "    temp = np.array(temp)\n",
    "    labels.append(temp)\n",
    "    true_boxes.append(data[i]['boxes'])\n",
    "\n",
    "# for image in os.listdir(picsFolder_path):\n",
    "#     images.append(image)\n",
    "print(\"we have\",len(images),\"images\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Resizing Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33402\n"
     ]
    }
   ],
   "source": [
    "resizedImages =[]\n",
    "for image in images:\n",
    "    resizedImages.append(cv2.resize(image,(100,75)))\n",
    "print(len(resizedImages))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Creating Images Copy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33402\n"
     ]
    }
   ],
   "source": [
    "imagesCopy = []\n",
    "for resizedimage in resizedImages:\n",
    "    imagesCopy.append(resizedimage)\n",
    "print(len(imagesCopy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Removing background from images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33402\n"
     ]
    }
   ],
   "source": [
    "backgroundremovedImages=[]\n",
    "\n",
    "for resizedimage in resizedImages:\n",
    "    #== Parameters\n",
    "    BLUR = 11\n",
    "    CANNY_THRESH_1 = 10\n",
    "    CANNY_THRESH_2 = 100\n",
    "    MASK_DILATE_ITER = 10\n",
    "    MASK_ERODE_ITER = 10\n",
    "    MASK_COLOR = (0.0,0.0,1.0) # In BGR format\n",
    "    grayimage = cv2.cvtColor(resizedimage,cv2.COLOR_BGR2GRAY)\n",
    "     #-- Edge detection\n",
    "    edges = cv2.Canny(grayimage, CANNY_THRESH_1, CANNY_THRESH_2)\n",
    "    edges = cv2.dilate(edges, None)\n",
    "    edges = cv2.erode(edges, None)\n",
    "     #-- Find contours in edges, sort by area\n",
    "    contour_info = []\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    if len(contours) > 0:\n",
    "        contour_info = []\n",
    "        for c in contours:\n",
    "            contour_info.append((\n",
    "                c,\n",
    "                cv2.isContourConvex(c),\n",
    "                cv2.contourArea(c),\n",
    "            ))\n",
    "        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)\n",
    "        max_contour = contour_info[0]\n",
    "        #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----\n",
    "        # Mask is black, polygon is white\n",
    "        mask = np.zeros(edges.shape)\n",
    "        cv2.fillConvexPoly(mask, max_contour[0], (255))\n",
    "        #-- Smooth mask, then blur it\n",
    "        mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)\n",
    "        mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)\n",
    "        mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)\n",
    "        mask_stack = np.dstack([mask]*3)    # Create 3-channel alpha mask\n",
    "        #-- Blend masked img into MASK_COLOR background\n",
    "        mask_stack  = mask_stack.astype('float32') / 255.0\n",
    "        masking= resizedimage.astype('float32') / 255.0\n",
    "        masked = (mask_stack * masking) + ((1-mask_stack) * MASK_COLOR)\n",
    "        masked = (masked * 255).astype('uint8')\n",
    "        backgroundremovedImages.append(masked)\n",
    "\n",
    "    else:\n",
    "        # Handle case when no contours are detected, for example:\n",
    "        backgroundremovedImages.append(resizedimage)\n",
    "\n",
    "print(len(backgroundremovedImages))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Converting Original Images copy from BGR to Grayscale  </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33402\n"
     ]
    }
   ],
   "source": [
    "greyImages = [] \n",
    "# Convert BGR to grayscale:\n",
    "for backgroundremovedimages in backgroundremovedImages:\n",
    "    greyImages.append(cv2.cvtColor(backgroundremovedimages, cv2.COLOR_BGR2GRAY)) \n",
    "print(len(greyImages))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sharpening Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33402\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sharpenedImages = []\n",
    "\n",
    "for greyimage in greyImages:\n",
    "    \n",
    "    # apply Gaussian blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(greyimage, (3, 3), 0)\n",
    "\n",
    "    # apply Laplacian filter to extract edges\n",
    "    laplacian = cv2.Laplacian(blurred, cv2.CV_64F)\n",
    "    laplacian = cv2.convertScaleAbs(laplacian)\n",
    "\n",
    "\n",
    "    # apply edge sharpening using the grayscale original image and the Laplacian edges\n",
    "    sharpened = cv2.convertScaleAbs(greyimage - laplacian)\n",
    "    sharpenedImages.append(sharpened)\n",
    "\n",
    "print(len(sharpenedImages))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adaptive Thresholding on Grayscale Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the adaptive thresholding:\n",
    "windowSize = 31\n",
    "windowConstant = -1\n",
    "\n",
    "binaryImages = []\n",
    "\n",
    "\n",
    "# Apply the threshold:\n",
    "for sharpenedimage in sharpenedImages:\n",
    "    binaryImages.append(cv2.adaptiveThreshold(sharpenedimage, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,\n",
    "                                        windowSize, windowConstant))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Perform Connected Components on the Binary Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredImages =[]\n",
    "\n",
    "# Perform Connected Components:\n",
    "for binaryimage in binaryImages:\n",
    "    componentsNumber, labeledImage, componentStats, componentCentroids = cv2.connectedComponentsWithStats(binaryimage,\n",
    "                                                                                                          connectivity=4)\n",
    "    # Set the minimum pixels for the area filter to filter connected components:\n",
    "    minArea = 20\n",
    "\n",
    "    # Get the indices/labels of the remaining components based on the area stat\n",
    "    # (skip the background component at index 0)\n",
    "    remainingComponentLabels = [i for i in range(1, componentsNumber) if componentStats[i][4] >= minArea]\n",
    "    # Filter the labeled pixels based on the remaining labels,\n",
    "    # assign pixel intensity to 255 (uint8) for the remaining pixels\n",
    "    # y3ni b3d de el mafood yfdal the largest connected components that hopefully contains the digits\n",
    "    # one problem is that background connected components may still exist\n",
    "    filteredImages.append(np.where(np.isin(labeledImage, remainingComponentLabels) == True, 255, 0).astype('uint8'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Median Filter  (not used)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medianImages=[]\n",
    "for filteredimage in filteredImages:\n",
    "    medianImages.append( cv2.medianBlur(filteredimage, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Morphological Operations - Closing on Filtered Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set kernel (structuring element) size:\n",
    "kernelSize = 3\n",
    "\n",
    "# Set operation iterations:\n",
    "opIterations = 1\n",
    "\n",
    "# Get the structuring element:\n",
    "maxKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernelSize, kernelSize))\n",
    "\n",
    "closingImages = []\n",
    "\n",
    "# Perform closing:\n",
    "for filteredimage in filteredImages:\n",
    "    closingImages.append(cv2.morphologyEx(filteredimage, cv2.MORPH_CLOSE, maxKernel, None, None, opIterations,\n",
    "                                cv2.BORDER_REFLECT101))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Opening on the Closed Images (not used)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set kernel (structuring element) size:\n",
    "kernelSize = 3\n",
    "\n",
    "# Set operation iterations:\n",
    "opIterations = 1\n",
    "\n",
    "# Get the structuring element:\n",
    "maxKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernelSize, kernelSize))\n",
    "\n",
    "openingImages = []\n",
    "\n",
    "# Perform closing:\n",
    "for closingimage in closingImages:\n",
    "    openingImages.append(cv2.morphologyEx(closingimage, cv2.MORPH_OPEN, maxKernel, None, None, opIterations,\n",
    "                                cv2.BORDER_REFLECT101))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Gaussian Blur (not used)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33402\n"
     ]
    }
   ],
   "source": [
    "smoothedImages =[]\n",
    "\n",
    "# perform smoothing\n",
    "for closingimage in closingImages:\n",
    "    smoothedImages.append(cv2.GaussianBlur(closingimage, (3, 3), 0))\n",
    "print(len(smoothedImages))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Unsharp masking (not used)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33402\n"
     ]
    }
   ],
   "source": [
    "unsharpenedImages=[]\n",
    "# Apply unsharp masking\n",
    "for smoothedimage in smoothedImages:\n",
    "    unsharpenedimage = cv2.GaussianBlur(smoothedimage, (5, 5), 0)\n",
    "    unsharpenedImages.append(cv2.addWeighted(smoothedimage, 1.5, unsharpenedimage, -0.5, 0))\n",
    "print(len(unsharpenedImages))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Canny Edge Detection on Smoothed Images </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33402\n"
     ]
    }
   ],
   "source": [
    "edgeImages =[]\n",
    "\n",
    "# perform smoothing\n",
    "for closingimage in closingImages:\n",
    "# perform canny edge detection:\n",
    "    edgeImages.append(cv2.Canny(smoothedimage, 100, 200))\n",
    "print(len(edgeImages))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Drawing bounding boxes on digits </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 0\n",
    "# Get each bounding box\n",
    "# Find the big contours on the filtered image:\n",
    "# for edgeimage in edgeImages:\n",
    "def draw_boxes(edgeimage, copyimage):\n",
    "    contours, hierarchy = cv2.findContours(edgeimage, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_poly = [None] * len(contours)\n",
    "    # The Bounding Rectangles will be stored here:\n",
    "    boundRect = []\n",
    "    # Alright, just look for the outer bounding boxes:\n",
    "    for i, c in enumerate(contours):\n",
    "        if hierarchy[0][i][3] == -1:\n",
    "            contours_poly[i] = cv2.approxPolyDP(c, 3, True)\n",
    "            boundRect.append(cv2.boundingRect(contours_poly[i]))\n",
    "    for i in range(len(boundRect)):\n",
    "\n",
    "        color = (0, 255, 0)\n",
    "        # filter contours according to the area of the rectangle (parameter re5em)\n",
    "        if ( int(boundRect[i][2])*int(boundRect[i][3])>100 and int(boundRect[i][2])*int(boundRect[i][3])<1000):\n",
    "            cv2.rectangle(copyimage, (int(boundRect[i][0]), int(boundRect[i][1])),\n",
    "                          (int(boundRect[i][0] + boundRect[i][2]), int(boundRect[i][1] + boundRect[i][3])), color, 2)\n",
    "    # index= index + 1\n",
    "    boxes= []\n",
    "            # Iterate through each bounding box\n",
    "    for i in range(len(boundRect)):\n",
    "        # Extract the coordinates and dimensions of the bounding box\n",
    "        x, y, w, h = boundRect[i]\n",
    "            \n",
    "        # Append the bounding box to the list in the format of a dictionary\n",
    "        boxes.append({'left': x, 'top': y, 'width': w, 'height': h})\n",
    "        \n",
    "    # Return the list of bounding boxes\n",
    "    return boxes  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Intersection over union </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(boxA, boxB):\n",
    "    # Calculate the intersection coordinates of the two bounding boxes\n",
    "    x1 = max(boxA[0], boxB[0])\n",
    "    y1 = max(boxA[1], boxB[1])\n",
    "    x2 = min(boxA[2], boxB[2])\n",
    "    y2 = min(boxA[3], boxB[3])\n",
    "    \n",
    "    # Compute the area of intersection rectangle\n",
    "    intersection_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
    "    \n",
    "    # Compute the area of both bounding boxes\n",
    "    boxA_area = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxB_area = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    \n",
    "    # Compute the union area\n",
    "    union_area = boxA_area + boxB_area - intersection_area\n",
    "    \n",
    "    # Compute the IOU\n",
    "    iou = intersection_area / union_area\n",
    "    \n",
    "    return iou\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculate accuracy  </h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>iouPicTest </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iouPicTest(truth, predicted, threshold1=0.5):\n",
    "    ious = []\n",
    "    for i in range(len(truth)):\n",
    "        for j in range(len(predicted)):\n",
    "            truth_box = [truth[i]['left'], truth[i]['top'], truth[i]['left'] + truth[i]['width'], truth[i]['top']+truth[i]['height']]\n",
    "            predicted_box = [predicted[j]['left'], predicted[j]['top'], predicted[j]['left']+predicted[j]['width'], predicted[j]['top']+predicted[j]['height']]\n",
    "            iou = box_iou(truth_box, predicted_box)\n",
    "            if iou >= threshold1:\n",
    "                ious.append(iou)\n",
    "    acc = np.average(ious) if ious else 0\n",
    "    return acc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Overlapping test </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapTest(truth, predicted, threshold=0.7):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for i in range(len(predicted)):\n",
    "        matched = False\n",
    "        for j in range(len(truth)):\n",
    "            truth_box = [truth[j]['left'], truth[j]['top'], truth[j]['left'] + truth[j]['width'], truth[j]['top'] + truth[j]['height']]\n",
    "            predicted_box = [predicted[i]['left'], predicted[i]['top'], predicted[i]['left'] + predicted[i]['width'], predicted[i]['top'] + predicted[i]['height']]\n",
    "            iou = box_iou(truth_box, predicted_box)\n",
    "            if iou >= threshold:\n",
    "                matched = True\n",
    "                break\n",
    "        if matched:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    if len(truth) > 0:\n",
    "        fn = len(truth) - tp\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1_score = 2 * precision * recall / (precision + recall)\n",
    "        return tp,precision, recall, f1_score\n",
    "    else:\n",
    "        return 0, 0, 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Show Images with bounding boxes </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m acc \u001b[39m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (\u001b[39m21\u001b[39m,\u001b[39m22\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     plt\u001b[39m.\u001b[39mimshow(images[i])\n\u001b[0;32m      4\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m      5\u001b[0m     boxes \u001b[39m=\u001b[39m draw_boxes(edgeImages[i],imagesCopy[i])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "calculating accuracy\n",
    "acc = []\n",
    "for i in range(len(images)):\n",
    "     #plt.imshow(images[i])\n",
    "     #plt.show()\n",
    "     boxes = draw_boxes(edgeImages[i],imagesCopy[i])\n",
    "     #tp,precision,recall,f1_score = overlapTest(edgeImages[i],imagesCopy[i])\n",
    "     acc.append(iouPicTest(true_boxes[i],boxes))\n",
    "     # print (acc)\n",
    "     #plt.imshow(imagesCopy[i])\n",
    "     #plt.show()\n",
    "    # cv2.imshow('image',imagesCopy[i])\n",
    "    # cv2.waitKey(0)\n",
    "print(np.average(acc)*100)\n",
    "# print(precision)\n",
    "# print(recall)\n",
    "# print(f1_score)\n",
    "# print(tp/len(true_boxes))\n",
    "#displaying ong of the output images\n",
    "for i in range(21,22):\n",
    "     plt.imshow(images[i])\n",
    "     plt.show()\n",
    "     boxes = draw_boxes(edgeImages[i],imagesCopy[i])\n",
    "     #tp,precision,recall,f1_score = overlapTest(edgeImages[i],imagesCopy[i])\n",
    "     #acc.append(iouPicTest(true_boxes[i],boxes))\n",
    "     # print (acc)\n",
    "     plt.imshow(imagesCopy[i])\n",
    "     plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
